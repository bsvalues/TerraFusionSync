This document provides the scaffold for the new "GIS Exports" plugin, following the established pattern.
*The `core_models.py` (with `GisExportJob`), `plugins/gis_export/schemas.py`, `plugins/__init__.py`, `metrics.py`, `tests/integration/test_gis_export_end_to_end.py` (stub), and CI block from the previous version of this document are still relevant. The main update here is to `plugins/gis_export/router.py`.*

#### 1. `terrafusion_sync/core_models.py` (Ensure `GisExportJob` is defined as previously)
*(Content from previous version of this document - `GisExportJob` model definition)*
```python
# terrafusion_sync/core_models.py
import datetime
from sqlalchemy import Column, String, Float, DateTime, Integer, Text, Boolean, JSON
from sqlalchemy.orm import declarative_base
from sqlalchemy.dialects.postgresql import UUID as PG_UUID
import uuid

# Assuming Base is already defined earlier in this file
# Base = declarative_base() # Remove if already defined

# ... (PropertyOperational, ValuationJob, ReportJob, MarketAnalysisJob models should be here) ...

class GisExportJob(Base):
    """
    Represents a GIS data export job, its status, parameters, and output location.
    """
    __tablename__ = "gis_export_jobs"

    job_id = Column(PG_UUID(as_uuid=True), primary_key=True, default=uuid.uuid4, comment="Unique ID for the GIS export job")
    export_format = Column(String, index=True, nullable=False, comment="Requested export format (e.g., GeoJSON, Shapefile, KML)")
    county_id = Column(String, index=True, nullable=False, comment="County ID for the data export")
    area_of_interest_json = Column(JSON, nullable=True, comment="JSON defining the area of interest (e.g., bounding box, polygon WKT/GeoJSON)")
    layers_json = Column(JSON, nullable=True, comment="JSON array of layer names or types to include in the export")
    
    status = Column(String, index=True, nullable=False, default="PENDING", comment="e.g., PENDING, RUNNING, COMPLETED, FAILED")
    message = Column(Text, nullable=True, comment="Status message or error details")
    
    parameters_json = Column(JSON, nullable=True, comment="Additional JSON object storing other parameters for the export")
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.datetime.utcnow, onupdate=datetime.datetime.utcnow)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)

    # Result Information
    result_file_location = Column(String, nullable=True, comment="Location of the generated export file (e.g., S3 path, downloadable link)")
    result_file_size_kb = Column(Integer, nullable=True)
    result_record_count = Column(Integer, nullable=True)

    def __repr__(self):
        return f"<GisExportJob(job_id='{self.job_id}', export_format='{self.export_format}', county_id='{self.county_id}', status='{self.status}')>"
```

#### 2. `terrafusion_sync/plugins/gis_export/schemas.py` (Ensure this is defined as previously)
*(Content from previous version of this document)*
```python
# terrafusion_sync/plugins/gis_export/schemas.py
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
import uuid
import datetime

class GisExportRunRequest(BaseModel):
    export_format: str = Field(..., example="GeoJSON", description="Desired output format (e.g., GeoJSON, Shapefile, KML).")
    county_id: str = Field(..., example="COUNTY01")
    area_of_interest: Optional[Dict[str, Any]] = Field(None, example={"type": "bbox", "coordinates": [-120.5, 46.0, -120.0, 46.5]}, description="Defines the spatial extent.")
    layers: List[str] = Field(..., example=["parcels", "zoning"], description="List of data layers to include.")
    parameters: Optional[Dict[str, Any]] = Field(None, example={"include_assessment_data": True}, description="Additional export parameters.")

class GisExportJobStatusResponse(BaseModel):
    job_id: uuid.UUID
    export_format: str
    county_id: str
    status: str
    message: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None 
    created_at: datetime.datetime
    updated_at: datetime.datetime
    started_at: Optional[datetime.datetime] = None
    completed_at: Optional[datetime.datetime] = None

class GisExportResultData(BaseModel):
    result_file_location: Optional[str] = None 
    result_file_size_kb: Optional[int] = None
    result_record_count: Optional[int] = None

class GisExportJobResultResponse(GisExportJobStatusResponse):
    result: Optional[GisExportResultData] = None
```

#### 3. `terrafusion_sync/plugins/gis_export/router.py` (Refined for Robustness)
```python
# terrafusion_sync/plugins/gis_export/router.py
from fastapi import APIRouter, Depends, HTTPException, status as fastapi_status, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy.orm import sessionmaker # To create session factory for background task
import logging
from typing import Optional, Dict, Any, List
import uuid
import asyncio
import time
import datetime

# Adjust import paths based on your project structure.
# Assuming connectors.postgres and core_models are two levels up from plugins/gis_export/
from ...connectors.postgres import get_db_session, AsyncSessionFactory # Import the global AsyncSessionFactory
from ...core_models import GisExportJob, PropertyOperational # Assuming PropertyOperational has geometry

# Import Prometheus metrics from the shared metrics.py module
try:
    from ...metrics import ( 
        GIS_EXPORT_JOBS_SUBMITTED_TOTAL,
        GIS_EXPORT_JOBS_COMPLETED_TOTAL,
        GIS_EXPORT_JOBS_FAILED_TOTAL,
        GIS_EXPORT_PROCESSING_DURATION_SECONDS
    )
except ImportError:
    print("Warning (gis_export.py): Could not import Prometheus metrics. Metrics will not be recorded.")
    class DummyCounter:
        def inc(self, amount=1): pass
        def labels(self, *args, **kwargs): return self
    class DummyHistogram:
        def observe(self, amount): pass
        def labels(self, *args, **kwargs): return self
    GIS_EXPORT_JOBS_SUBMITTED_TOTAL = DummyCounter()
    GIS_EXPORT_JOBS_COMPLETED_TOTAL = DummyCounter()
    GIS_EXPORT_JOBS_FAILED_TOTAL = DummyCounter()
    GIS_EXPORT_PROCESSING_DURATION_SECONDS = DummyHistogram()

# Import Pydantic schemas from the local schemas.py file
from .schemas import (
    GisExportRunRequest, 
    GisExportJobStatusResponse, 
    GisExportResultData,
    GisExportJobResultResponse
)

logger = logging.getLogger(__name__)
router = APIRouter() # Prefix "/gis-export" will be added by plugins/__init__.py

# --- Background Task for GIS Export Processing ---
async def _process_gis_export_job(
    job_id: uuid.UUID, 
    export_format: str, 
    county_id: str, 
    area_of_interest: Optional[Dict[str, Any]],
    layers: List[str],
    parameters: Optional[Dict[str, Any]]
    # Removed db_session_factory, will use global AsyncSessionFactory
):
    start_process_time = time.monotonic()
    job_final_status_for_metric = "UNKNOWN_FAILURE" # Default for metrics if early exit
    logger.info(f"GisExportJob {job_id}: Background task initiated.")

    if not AsyncSessionFactory:
        logger.error(f"GisExportJob {job_id}: Global AsyncSessionFactory is not available. Cannot proceed with background DB operations.")
        # This is a critical configuration error.
        return

    async with AsyncSessionFactory() as db: # Create a new session for this background task
        logger.info(f"GisExportJob {job_id}: DB session acquired for background task. Processing format '{export_format}', county '{county_id}'.")
        
        job_to_update: Optional[GisExportJob] = None # Ensure job_to_update is always defined
        try:
            # Fetch the job record to update
            result = await db.execute(select(GisExportJob).where(GisExportJob.job_id == job_id))
            job_to_update = result.scalars().first()

            if not job_to_update:
                logger.error(f"GisExportJob {job_id}: Not found in DB at start of background task. Cannot update status.")
                GIS_EXPORT_JOBS_FAILED_TOTAL.labels(county_id=county_id, export_format=export_format, failure_reason="job_not_found_in_bg").inc()
                return

            job_to_update.status = "RUNNING"
            job_to_update.started_at = datetime.datetime.utcnow()
            job_to_update.updated_at = datetime.datetime.utcnow()
            await db.commit()
            await db.refresh(job_to_update) # Refresh to get updated state
            logger.info(f"GisExportJob {job_id}: Status updated to RUNNING in DB.")

            # --- Actual GIS Export Logic Placeholder ---
            logger.info(f"GisExportJob {job_id}: Simulating GIS export of format '{export_format}' for layers {layers} with AOI {area_of_interest} and params: {parameters}")
            # TODO: Implement actual GIS export logic.
            # This would involve:
            # 1. Parsing area_of_interest and layers.
            # 2. Querying PropertyOperational (and potentially other tables or a PostGIS DB)
            #    using spatial filters based on AOI. Need to ensure PropertyOperational has geometry data
            #    or that you have a separate PostGIS connector and database.
            #    Example (very simplified, assumes PropertyOperational has 'geometry' and 'attributes' fields):
            #    stmt = select(PropertyOperational.geometry, PropertyOperational.attributes_json).where(
            #        PropertyOperational.county_id == county_id,
            #        # Add spatial filter for AOI here using PostGIS functions if applicable
            #        # e.g., ST_Intersects(PropertyOperational.geom_column, ST_GeomFromGeoJSON(str(area_of_interest)))
            #    )
            #    data_to_export = await db.execute(stmt)
            #    records = data_to_export.all()
            # 3. Selecting relevant attributes for the specified layers.
            # 4. Converting data to the requested export_format (GeoJSON, Shapefile, KML).
            #    - For Shapefile/KML, this might involve creating multiple files and zipping them.
            #    - Libraries like GeoPandas, Fiona, shapely would be useful here.
            # 5. Storing the resulting file(s) (e.g., to S3 or a temporary file store).
            
            await asyncio.sleep(6) # Simulate processing time (adjust as needed for testing timeouts)
            
            # --- End Actual GIS Export Logic Placeholder ---

            if export_format == "FAIL_FORMAT_SIM": # For testing failure path
                job_to_update.status = "FAILED"
                job_to_update.message = "Simulated GIS export failure due to requested format."
                GIS_EXPORT_JOBS_FAILED_TOTAL.labels(county_id=county_id, export_format=export_format, failure_reason="simulated_format_failure").inc()
            else:
                job_to_update.status = "COMPLETED"
                job_to_update.message = f"GIS export ({export_format}) completed successfully."
                job_to_update.result_file_location = f"/gis_exports_output/{county_id}/{job_id}_{'_'.join(layers)}.{export_format.lower().replace('shapefile','zip')}"
                job_to_update.result_file_size_kb = 5120 # Simulated
                job_to_update.result_record_count = 2500 # Simulated
                GIS_EXPORT_JOBS_COMPLETED_TOTAL.labels(county_id=county_id, export_format=export_format).inc()
            
            job_final_status_for_metric = job_to_update.status
            job_to_update.completed_at = datetime.datetime.utcnow()
            job_to_update.updated_at = datetime.datetime.utcnow()
            await db.commit()
            logger.info(f"GisExportJob {job_id}: Final status '{job_to_update.status}' and results committed to DB.")

        except Exception as e:
            job_final_status_for_metric = "EXCEPTION_FAILURE"
            logger.error(f"GisExportJob {job_id}: Unhandled exception during background GIS export: {e}", exc_info=True)
            GIS_EXPORT_JOBS_FAILED_TOTAL.labels(county_id=county_id, export_format=export_format, failure_reason="processing_exception").inc()
            if job_to_update: # If job was fetched
                try:
                    job_to_update.status = "FAILED"
                    job_to_update.message = f"Processing error: {str(e)[:490]}" # Truncate for DB
                    job_to_update.completed_at = datetime.datetime.utcnow()
                    job_to_update.updated_at = datetime.datetime.utcnow()
                    await db.commit()
                    logger.info(f"GisExportJob {job_id}: Marked as FAILED in DB due to processing exception.")
                except Exception as db_error:
                    logger.error(f"GisExportJob {job_id}: CRITICAL - Failed to update job status to FAILED in DB after processing error: {db_error}", exc_info=True)
        finally:
            duration = time.monotonic() - start_process_time
            GIS_EXPORT_PROCESSING_DURATION_SECONDS.labels(county_id=county_id, export_format=export_format).observe(duration)
            logger.info(f"GisExportJob {job_id}: Background task finished. Duration: {duration:.2f}s. Final reported status for metrics: {job_final_status_for_metric}")


# --- API Endpoints for GIS Export Plugin ---
@router.post("/run", response_model=GisExportJobStatusResponse, status_code=fastapi_status.HTTP_202_ACCEPTED)
async def run_gis_export_job(
    request_data: GisExportRunRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db_session)
):
    job_id = uuid.uuid4()
    current_time = datetime.datetime.utcnow()
    new_job = GisExportJob(
        job_id=job_id,
        export_format=request_data.export_format,
        county_id=request_data.county_id,
        area_of_interest_json=request_data.area_of_interest,
        layers_json=request_data.layers,
        parameters_json=request_data.parameters,
        status="PENDING",
        message="GIS export job accepted and queued.",
        created_at=current_time,
        updated_at=current_time
    )
    
    try:
        db.add(new_job)
        await db.commit()
        await db.refresh(new_job)
        logger.info(f"GisExportJob {new_job.job_id} created in DB for format '{new_job.export_format}', county '{new_job.county_id}'.")
        GIS_EXPORT_JOBS_SUBMITTED_TOTAL.labels(
            county_id=request_data.county_id, 
            export_format=request_data.export_format,
            status_on_submit="PENDING"
        ).inc()
    except Exception as e:
        logger.error(f"Failed to create GIS export job in DB for {request_data.export_format} / {request_data.county_id}: {e}", exc_info=True)
        raise HTTPException(status_code=fastapi_status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to initiate GIS export job.")
    
    if not AsyncSessionFactory:
        logger.critical("AsyncSessionFactory is not initialized. Background task for GIS export cannot be scheduled reliably.")
        # Fallback, but this indicates a setup issue in connectors/postgres.py
        temp_session_factory = sessionmaker(bind=db.get_bind(), class_=AsyncSession, expire_on_commit=False)
        background_tasks.add_task(
            _process_gis_export_job, 
            new_job.job_id, new_job.export_format, new_job.county_id,
            new_job.area_of_interest_json, new_job.layers_json, new_job.parameters_json,
            temp_session_factory
        )
    else:
        background_tasks.add_task(
            _process_gis_export_job, 
            new_job.job_id, new_job.export_format, new_job.county_id,
            new_job.area_of_interest_json, new_job.layers_json, new_job.parameters_json,
            AsyncSessionFactory # Pass the global factory
        )
    return GisExportJobStatusResponse.model_validate(new_job)

@router.get("/status/{job_id_str}", response_model=GisExportJobStatusResponse)
async def get_gis_export_job_status(job_id_str: str, db: AsyncSession = Depends(get_db_session)):
    try:
        job_uuid = uuid.UUID(job_id_str)
    except ValueError:
        raise HTTPException(status_code=fastapi_status.HTTP_400_BAD_REQUEST, detail="Invalid job_id format.")
    
    logger.debug(f"Fetching status for GisExportJob ID: {job_uuid}")
    result = await db.execute(select(GisExportJob).where(GisExportJob.job_id == job_uuid))
    job = result.scalars().first()
    if not job:
        logger.warning(f"GisExportJob status request for unknown ID: {job_uuid}")
        raise HTTPException(status_code=fastapi_status.HTTP_404_NOT_FOUND, detail=f"GIS export job '{job_uuid}' not found.")
    
    # Reconstruct combined parameters for the response model
    response_parameters = {
        "area_of_interest": job.area_of_interest_json,
        "layers": job.layers_json,
        **(job.parameters_json if job.parameters_json else {})
    }
    response_data = job.__dict__
    response_data["parameters"] = response_parameters # Add the combined parameters field for Pydantic validation
    
    return GisExportJobStatusResponse.model_validate(response_data)


@router.get("/results/{job_id_str}", response_model=GisExportJobResultResponse)
async def get_gis_export_job_results(job_id_str: str, db: AsyncSession = Depends(get_db_session)):
    try:
        job_uuid = uuid.UUID(job_id_str)
    except ValueError:
        raise HTTPException(status_code=fastapi_status.HTTP_400_BAD_REQUEST, detail="Invalid job_id format.")

    logger.debug(f"Fetching results for GisExportJob ID: {job_uuid}")
    result = await db.execute(select(GisExportJob).where(GisExportJob.job_id == job_uuid))
    job = result.scalars().first()
    if not job:
        logger.warning(f"GisExportJob results request for unknown ID: {job_uuid}")
        raise HTTPException(status_code=fastapi_status.HTTP_404_NOT_FOUND, detail=f"GIS export job '{job_uuid}' not found.")

    # Prepare the base status data for the response
    job_status_data_dict = GisExportJobStatusResponse.model_validate(job).model_dump(exclude_none=True)
    # Reconstruct combined parameters for the response model
    job_status_data_dict["parameters"] = {
        "area_of_interest": job.area_of_interest_json,
        "layers": job.layers_json,
        **(job.parameters_json if job.parameters_json else {})
    }
    
    export_result_data_obj = None
    if job.status == "COMPLETED":
        export_result_data_obj = GisExportResultData(
            result_file_location=job.result_file_location,
            result_file_size_kb=job.result_file_size_kb,
            result_record_count=job.result_record_count
        )
        logger.info(f"GisExportJob {job_uuid} results retrieved. File location: {job.result_file_location}")
    else:
        logger.info(f"GisExportJob {job_uuid} not yet completed or failed. Status: {job.status}")
        
    return GisExportJobResultResponse(**job_status_data_dict, result=export_result_data_obj)

```

#### 4. `terrafusion_sync/plugins/__init__.py` (Ensure GIS Export router is registered)
*(Content from previous version of this document - ensure `gis_export_router` is imported and included)*
```python
# terrafusion_sync/plugins/__init__.py
from fastapi import APIRouter
import logging 

logger = logging.getLogger(__name__)

# Import routers from each plugin module
try:
    from .valuation import router as valuation_router
    logger.info("Valuation plugin router imported.")
except ImportError:
    logger.warning("Could not import valuation_router.")
    valuation_router = None

try:
    from .reporting import router as reporting_router
    logger.info("Reporting plugin router imported.")
except ImportError:
    logger.warning("Could not import reporting_router.")
    reporting_router = None

try:
    from .market_analysis.router import router as market_analysis_router
    logger.info("Market Analysis plugin router imported.")
except ImportError:
    logger.warning("Could not import market_analysis_router.")
    market_analysis_router = None

try:
    from .gis_export.router import router as gis_export_router 
    logger.info("GIS Export plugin router imported.")
except ImportError as e:
    logger.error(f"Failed to import gis_export_router: {e}", exc_info=True)
    gis_export_router = None

all_plugin_routers = APIRouter()

if valuation_router:
    all_plugin_routers.include_router(valuation_router, prefix="/valuation", tags=["Valuation Services"])
if reporting_router:
    all_plugin_routers.include_router(reporting_router, prefix="/reporting", tags=["Reporting Services"])
if market_analysis_router:
    all_plugin_routers.include_router(market_analysis_router, prefix="/market-analysis", tags=["Market Analysis Services"])
if gis_export_router: 
    all_plugin_routers.include_router(gis_export_router, prefix="/gis-export", tags=["GIS Export Services"])
    logger.info("GIS Export plugin router included in all_plugin_routers.")
else:
    logger.warning("GIS Export plugin router was not available to be included.")


def get_all_plugin_routers_instance():
    return all_plugin_routers
```

#### 5. `terrafusion_sync/metrics.py` (Ensure GIS Export metrics are defined)
*(Content from previous version of this document - ensure `GIS_EXPORT_*` metrics are defined)*
```python
# terrafusion_sync/metrics.py
from prometheus_client import Counter, Histogram

# ... (Valuation, Reporting, Market Analysis metrics) ...

# --- GIS Export Plugin Metrics ---
GIS_EXPORT_JOBS_SUBMITTED_TOTAL = Counter(
    'gis_export_jobs_submitted_total',
    'Total number of GIS export jobs submitted.',
    ['county_id', 'export_format', 'status_on_submit']
)
GIS_EXPORT_JOBS_COMPLETED_TOTAL = Counter(
    'gis_export_jobs_completed_total',
    'Total number of GIS export jobs completed successfully.',
    ['county_id', 'export_format']
)
GIS_EXPORT_JOBS_FAILED_TOTAL = Counter(
    'gis_export_jobs_failed_total',
    'Total number of GIS export jobs that failed.',
    ['county_id', 'export_format', 'failure_reason']
)
GIS_EXPORT_PROCESSING_DURATION_SECONDS = Histogram(
    'gis_export_processing_duration_seconds',
    'Histogram of GIS export job processing time in seconds.',
    ['county_id', 'export_format'],
    buckets=[5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0, 1800.0, 3600.0, float('inf')] 
)
```